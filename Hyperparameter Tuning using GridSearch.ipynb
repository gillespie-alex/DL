{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cbcc16",
   "metadata": {},
   "source": [
    "# Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25b3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "df = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# Split the dataset\n",
    "X = df[:, 0:8]\n",
    "Y = df[:,-1]\n",
    "\n",
    "# Fix the random seed for reproducability\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca386a",
   "metadata": {},
   "source": [
    "## Keras Models in scikit-learn\n",
    "\n",
    "Keras models must be wrapped in either **KerasClassifier** or **KerasRegressor** class from the **SciKeras** module. To utilize these wrappers we need to define a function that creates and returns the Keras sequential model, then pass this function to the `model` argument when construction the **KerasClassifier** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a model as mentioned above\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dfc1f",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2d4c6",
   "metadata": {},
   "source": [
    "The constructor for the KerasClassifier class can take new arguments that can be passed to your custom `create_model()` function. These new args must also be defined in the signature of your `create_model()` function with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c4868",
   "metadata": {},
   "source": [
    "## Tuning Batch Size and # of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9645ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05746b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarizing the results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"{round(mean, 6)} {round(stdev, 6)} with {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b447aba",
   "metadata": {},
   "source": [
    "## Tuning Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f705f47",
   "metadata": {},
   "source": [
    "Using the above results we will now test for the optimal learning rate and momentum for the SGD optimizer.  Momentum controls how much to let the previous update influence the current weight update. For this step we will be assuming that Stochastic Gradient Descent is the most optimal optimizer to use which in most cases will be correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02872a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new create_model() function as this one won't include the .compile() call \n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa88116",
   "metadata": {},
   "source": [
    "In the above `create_model()` we don't include the .compile() unlike earlier as its better to leave the optimizer for a Keras model to the KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, \n",
    "                        loss=\"binary_crossentropy\", \n",
    "                        optimizer=\"SGD\", \n",
    "                        epochs=100, \n",
    "                        batch_size=10, \n",
    "                        verbose=0)\n",
    "\n",
    "lr = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "# With the SciKeras wrapper, we will route the parameters \n",
    "# to the optimizer with the prefix optimizer__\n",
    "param_grid = dict(optimizer__learning_rate=lr, optimizer__momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f7670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize the Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c750454",
   "metadata": {},
   "source": [
    "## Tuning Network Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e594d",
   "metadata": {},
   "source": [
    "Weight initialization refers to the process of randomly initializing the network's \n",
    "weights and biases. There are many methods to doing this ie. HeNormal, GlorotUniform, RandomUniform etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new create_model() function to accept the initialization technique as parameter\n",
    "def create_model(init_mode='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Defining the GridSearch parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero','glorot_normal',\n",
    "            'glorot_uniform', 'he_normal','he_uniform']\n",
    "param_grid = dict(model__init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f5057",
   "metadata": {},
   "source": [
    "## Tuning the Neuron Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd4b2d",
   "metadata": {},
   "source": [
    "The activation function controls the non-linearity of individual neurons and when to fire. Normally the `rectifier` or `sigmoid` are most popular. The GridSearch will only be testing the activation function for the hidden layers as the output layer needs to have an activation function used for binary classification. \n",
    "\n",
    "Note that generally each activation function will perform differently due to range of data that the neuron will be inputted. Some activation functions need the input to be standardized and others don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb474a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(model__activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78386646",
   "metadata": {},
   "source": [
    "## Tuning Dropout Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195f34a",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique where randomly selected neurons are ignored during training. They are \"dropped-out\" randomly. This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "As the neural network learns, neuron weights settle into their context  within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliance on context for a specific neuron during training is referred to complex *co-adaptations*.\n",
    "\n",
    "If neurons are randomly dropped out of the network during training, than other neurons will have to step in and handle the representation required to make predictions for the missing neurons. By doing this no neuron will be too independent of its neighboring neurons. This is believed to result in multiple independent internal reperesentation being learned by the network.\n",
    "\n",
    "The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n",
    "[Credit](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13caa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def create_model(dropout_rate, weight_constraint):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), kernel_initializer='uniform', activation='linear', kernel_constraint=MaxNorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb5a19",
   "metadata": {},
   "source": [
    "## Tuning the Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91874a2",
   "metadata": {},
   "source": [
    "The number of neuruons in the hidden layers control the representation capacity of the network, at least at that point in the topology. A large enough single layer (in theory) can approximate any other neural network. For this case we will be try values between 1 and 30 in steps of 5. The larger the network the more training will be needed; and ideally the batch size and number of epochs should be optimized with the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(8,), kernel_initializer='uniform', activation='linear', kernel_constraint=MaxNorm(4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(model__neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058fc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ff3c1",
   "metadata": {},
   "source": [
    "[Credit](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
